{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4548628-c3d4-48c9-8cca-2afa21f1f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b9aa62-389a-40d2-8f39-5905178f044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://bo.um.warszawa.pl\"\n",
    "PROJECTS_LIST_URL = BASE_URL + \"/projects?page={page_number}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee125788-5bef-4afc-ae1c-4b346667c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page {page_number}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_page_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # You can customize the parsing logic based on the structure of the webpage\n",
    "    return soup\n",
    "\n",
    "def get_projects_list_grid(soup):\n",
    "    projects_list_grid = soup.find('tbody', attrs={\"id\": \"projects-list\"})\n",
    "    return projects_list_grid\n",
    "\n",
    "def extract_project_links(projects_list_grid):\n",
    "    project_links = []\n",
    "    rows = projects_list_grid.find_all('tr')\n",
    "    for row in rows:\n",
    "        title_column = row.find('td', class_='text-left title-column')\n",
    "        if title_column:\n",
    "            link = title_column.find('a')\n",
    "            if link:\n",
    "                project_links.append(link['href'])\n",
    "    return project_links\n",
    "\n",
    "def get_full_urls(project_links, base_url):\n",
    "    full_urls = [base_url + link for link in project_links]\n",
    "    return full_urls\n",
    "\n",
    "def get_project_info(soup, strong_text):\n",
    "    strong_tag = soup.find('strong', string=strong_text)\n",
    "    if strong_tag:\n",
    "        next_tag = strong_tag.find_next_sibling()\n",
    "        if next_tag:\n",
    "            return next_tag.text.strip()\n",
    "    return None\n",
    "\n",
    "def get_all_subsequent_paragraphs(soup, strong_text):\n",
    "    strong_tag = soup.find('strong', string=strong_text)\n",
    "    paragraphs = []\n",
    "    if strong_tag:\n",
    "        next_sibling = strong_tag.find_next_sibling()\n",
    "        while next_sibling and next_sibling.name == 'p':\n",
    "            paragraphs.append(next_sibling.text.strip())\n",
    "            next_sibling = next_sibling.find_next_sibling()\n",
    "    return paragraphs\n",
    "\n",
    "def get_list_elements(soup, strong_text):\n",
    "    strong_tag = soup.find('strong', string=strong_text)\n",
    "    html_list = strong_tag.find_next_sibling()\n",
    "    res = []\n",
    "    if html_list.name == 'ul':\n",
    "        for el in html_list:\n",
    "            text = el.text.strip()\n",
    "            if text:\n",
    "                res.append(text)\n",
    "    return res\n",
    "\n",
    "def get_text_after_strong_tag(soup, strong_text):\n",
    "    strong_tag = soup.find('strong', string=strong_text)\n",
    "    if strong_tag:\n",
    "        next_element = strong_tag.next_sibling\n",
    "        while next_element and next_element.name is None:\n",
    "            text = next_element.strip()\n",
    "            if text:\n",
    "                return text\n",
    "            next_element = next_element.next_sibling\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f42023e-0e24-4e5e-8ddc-446a830ff1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing project list #1\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #2\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #3\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #4\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #5\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #6\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #7\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #8\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #9\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "Processing project list #10\n",
      " | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data = []\n",
    "\n",
    "page_number = 1\n",
    "project_list_html_content = get_page_content(PROJECTS_LIST_URL.format(page_number=page_number))\n",
    "while project_list_html_content:\n",
    "    print(f\"Processing project list #{page_number}\")\n",
    "    project_list_parsed_content = parse_page_content(project_list_html_content)\n",
    "    projects_list_grid = get_projects_list_grid(project_list_parsed_content)\n",
    "    project_url_sufix = extract_project_links(projects_list_grid)\n",
    "    project_full_urls = get_full_urls(project_url_sufix, BASE_URL)\n",
    "    for project_full_url in project_full_urls:\n",
    "        print(\" |\", end='')\n",
    "        project_page_html = get_page_content(project_full_url)\n",
    "        project_page_content = parse_page_content(project_page_html)\n",
    "        proj_ = {\n",
    "            \"localization\": get_project_info(project_page_content, 'Lokalizacja projektu - ulica i nr / rejon ulic w Warszawie:'),\n",
    "            \"short_desc\": get_project_info(project_page_content, \"SkrÃ³cony opis projektu:\"),\n",
    "            \"long_desc\": get_all_subsequent_paragraphs(project_page_content, \"Opis projektu:\"),\n",
    "            \"categories\": get_list_elements(project_page_content, \"Kategoria tematyczna projektu:\"),\n",
    "            \"beneficiaries\": get_list_elements(project_page_content, \"Potencjalni odbiorcy projektu:\"),\n",
    "            \"est_cost\": get_text_after_strong_tag(project_page_content, \"Szacunkowy koszt realizacji projektu:\"),\n",
    "        }\n",
    "        scraped_data.append(proj_)\n",
    "    print()\n",
    "    page_number += 1\n",
    "    if page_number > 10:\n",
    "        break\n",
    "    project_list_html_content = get_page_content(PROJECTS_LIST_URL.format(page_number=page_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a604359-34c9-4e5e-b5b9-64bfc4a8d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total projects scraped: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total projects scraped: {len(scraped_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b392e264-c53c-4b79-8d0f-4aa8ef86df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"scraped_data.json\"\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(scraped_data, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
